#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
5Kæ¨¡å‹æ€§èƒ½æµ‹è¯•è„šæœ¬
"""

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import os
import json
from PIL import Image
import logging

# å¯¼å…¥æ¨¡å‹ç±»
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from train_fast_local import LightweightGenderBiasModel, FastInstagramDataset

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

class Model5KEvaluator:
    def __init__(self, model_path, csv_file, database_path):
        self.model_path = model_path
        self.csv_file = csv_file
        self.database_path = database_path
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        
    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        logger.info(f"åŠ è½½5Kæ¨¡å‹: {self.model_path}")
        
        # åˆ›å»ºæ¨¡å‹
        self.model = LightweightGenderBiasModel()
        
        # åŠ è½½æƒé‡
        checkpoint = torch.load(self.model_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        logger.info("âœ… 5Kæ¨¡å‹åŠ è½½æˆåŠŸ")
        
    def prepare_test_data(self, test_size=1000):
        """å‡†å¤‡æµ‹è¯•æ•°æ®"""
        logger.info(f"å‡†å¤‡æµ‹è¯•æ•°æ® ({test_size}æ ·æœ¬)")
        
        # å¯¼å…¥tokenizer
        from transformers import DistilBertTokenizer
        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        dataset = FastInstagramDataset(
            csv_file=self.csv_file,
            database_path=self.database_path,
            tokenizer=tokenizer,
            max_samples=test_size,
            max_length=64
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        test_loader = torch.utils.data.DataLoader(
            dataset, 
            batch_size=16, 
            shuffle=False, 
            num_workers=0
        )
        
        return test_loader, dataset
        
    def evaluate_model(self, test_size=1000):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        logger.info("ğŸ” å¼€å§‹æ¨¡å‹è¯„ä¼°")
        
        # å‡†å¤‡æ•°æ®
        test_loader, dataset = self.prepare_test_data(test_size)
        
        predictions = []
        targets = []
        
        logger.info(f"åœ¨{test_size}ä¸ªæ ·æœ¬ä¸Šè¯„ä¼°æ¨¡å‹...")
        
        with torch.no_grad():
            for batch_idx, (images, input_ids, attention_mask, scores) in enumerate(test_loader):
                images = images.to(self.device)
                input_ids = input_ids.to(self.device)
                attention_mask = attention_mask.to(self.device)
                scores = scores.to(self.device)
                
                # é¢„æµ‹
                outputs = self.model(images, input_ids, attention_mask)
                
                # åå½’ä¸€åŒ–åˆ°0-10èŒƒå›´
                pred_scores = outputs.cpu().numpy() * 10.0
                true_scores = scores.cpu().numpy() * 10.0
                
                predictions.extend(pred_scores.flatten())
                targets.extend(true_scores.flatten())
                
                if (batch_idx + 1) % 20 == 0:
                    logger.info(f"  å·²å¤„ç† {(batch_idx + 1) * 16} / {test_size} æ ·æœ¬")
        
        predictions = np.array(predictions)
        targets = np.array(targets)
        
        # è®¡ç®—æŒ‡æ ‡
        mae = mean_absolute_error(targets, predictions)
        rmse = np.sqrt(mean_squared_error(targets, predictions))
        r2 = r2_score(targets, predictions)
        
        # è®¡ç®—å‡†ç¡®ç‡ï¼ˆè¯¯å·®<=1, <=2çš„æ¯”ä¾‹ï¼‰
        errors = np.abs(predictions - targets)
        acc_1 = np.mean(errors <= 1.0) * 100
        acc_2 = np.mean(errors <= 2.0) * 100
        
        logger.info("ğŸ“Š 5Kæ¨¡å‹è¯„ä¼°ç»“æœ:")
        logger.info(f"  ğŸ“ˆ MAE (å¹³å‡ç»å¯¹è¯¯å·®): {mae:.4f}")
        logger.info(f"  ğŸ“ˆ RMSE (å‡æ–¹æ ¹è¯¯å·®): {rmse:.4f}")
        logger.info(f"  ğŸ“ˆ RÂ² (å†³å®šç³»æ•°): {r2:.4f}")
        logger.info(f"  ğŸ¯ è¯¯å·®â‰¤1åˆ†å‡†ç¡®ç‡: {acc_1:.2f}%")
        logger.info(f"  ğŸ¯ è¯¯å·®â‰¤2åˆ†å‡†ç¡®ç‡: {acc_2:.2f}%")
        
        return {
            'predictions': predictions,
            'targets': targets,
            'mae': mae,
            'rmse': rmse,
            'r2': r2,
            'acc_1': acc_1,
            'acc_2': acc_2
        }
    
    def create_evaluation_plots(self, results):
        """åˆ›å»ºè¯„ä¼°å›¾è¡¨"""
        logger.info("ğŸ“Š ç”Ÿæˆè¯„ä¼°å›¾è¡¨")
        
        predictions = results['predictions']
        targets = results['targets']
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('5Kæ ·æœ¬æ¨¡å‹æ€§èƒ½è¯„ä¼°', fontsize=16, fontweight='bold')
        
        # 1. é¢„æµ‹vsçœŸå®å€¼æ•£ç‚¹å›¾
        ax1 = axes[0, 0]
        ax1.scatter(targets, predictions, alpha=0.5, s=20)
        ax1.plot([0, 10], [0, 10], 'r--', linewidth=2)
        ax1.set_xlabel('çœŸå®åˆ†æ•°')
        ax1.set_ylabel('é¢„æµ‹åˆ†æ•°')
        ax1.set_title(f'é¢„æµ‹ vs çœŸå®å€¼\nMAE: {results["mae"]:.3f}, RÂ²: {results["r2"]:.3f}')
        ax1.grid(True, alpha=0.3)
        
        # 2. è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾
        ax2 = axes[0, 1]
        errors = np.abs(predictions - targets)
        ax2.hist(errors, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        ax2.axvline(results['mae'], color='red', linestyle='--', label=f'MAE: {results["mae"]:.3f}')
        ax2.set_xlabel('ç»å¯¹è¯¯å·®')
        ax2.set_ylabel('é¢‘æ¬¡')
        ax2.set_title('è¯¯å·®åˆ†å¸ƒ')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. æ®‹å·®å›¾
        ax3 = axes[1, 0]
        residuals = predictions - targets
        ax3.scatter(targets, residuals, alpha=0.5, s=20)
        ax3.axhline(y=0, color='r', linestyle='--')
        ax3.set_xlabel('çœŸå®åˆ†æ•°')
        ax3.set_ylabel('æ®‹å·® (é¢„æµ‹-çœŸå®)')
        ax3.set_title('æ®‹å·®åˆ†æ')
        ax3.grid(True, alpha=0.3)
        
        # 4. åˆ†æ•°åŒºé—´å‡†ç¡®ç‡
        ax4 = axes[1, 1]
        score_ranges = [(0, 2), (2, 4), (4, 6), (6, 8), (8, 10)]
        range_accs = []
        range_labels = []
        
        for low, high in score_ranges:
            mask = (targets >= low) & (targets < high)
            if np.sum(mask) > 0:
                range_errors = errors[mask]
                acc = np.mean(range_errors <= 1.0) * 100
                range_accs.append(acc)
                range_labels.append(f'{low}-{high}')
        
        bars = ax4.bar(range_labels, range_accs, color='lightcoral', alpha=0.7)
        ax4.set_xlabel('åˆ†æ•°åŒºé—´')
        ax4.set_ylabel('å‡†ç¡®ç‡ (%)')
        ax4.set_title('å„åˆ†æ•°åŒºé—´å‡†ç¡®ç‡ (è¯¯å·®â‰¤1)')
        ax4.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, acc in zip(bars, range_accs):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height + 1,
                    f'{acc:.1f}%', ha='center', va='bottom')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        plot_path = 'model_5k_evaluation.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        logger.info(f"ğŸ“Š è¯„ä¼°å›¾è¡¨å·²ä¿å­˜: {plot_path}")
        plt.show()
        
    def compare_with_2k_model(self):
        """ä¸2Kæ¨¡å‹ç»“æœå¯¹æ¯”"""
        logger.info("ğŸ“Š ä¸2Kæ¨¡å‹å¯¹æ¯”")
        
        # 2Kæ¨¡å‹çš„ç»“æœ (ä»ä¹‹å‰çš„è®­ç»ƒä¸­è·å–)
        model_2k_results = {
            'mae': 1.31,
            'r2': 0.38,
            'acc_1': 45.2,
            'acc_2': 80.4,
            'training_time': 13.8  # åˆ†é’Ÿ
        }
        
        # 5Kæ¨¡å‹çš„ç»“æœ
        results_5k = self.evaluate_model(test_size=1000)
        model_5k_results = {
            'mae': results_5k['mae'],
            'r2': results_5k['r2'],
            'acc_1': results_5k['acc_1'],
            'acc_2': results_5k['acc_2'],
            'training_time': 17.5  # åˆ†é’Ÿ
        }
        
        # æ‰“å°å¯¹æ¯”
        print("\n" + "="*60)
        print("ğŸ“Š 2K vs 5K æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
        print("="*60)
        print(f"{'æŒ‡æ ‡':<15} {'2Kæ¨¡å‹':<12} {'5Kæ¨¡å‹':<12} {'æ”¹å–„':<12}")
        print("-" * 60)
        
        # MAEå¯¹æ¯”
        mae_improvement = ((model_2k_results['mae'] - model_5k_results['mae']) / model_2k_results['mae']) * 100
        print(f"{'MAE':<15} {model_2k_results['mae']:<12.3f} {model_5k_results['mae']:<12.3f} {mae_improvement:+.1f}%")
        
        # RÂ²å¯¹æ¯”
        r2_improvement = ((model_5k_results['r2'] - model_2k_results['r2']) / model_2k_results['r2']) * 100
        print(f"{'RÂ²':<15} {model_2k_results['r2']:<12.3f} {model_5k_results['r2']:<12.3f} {r2_improvement:+.1f}%")
        
        # å‡†ç¡®ç‡å¯¹æ¯”
        acc1_improvement = model_5k_results['acc_1'] - model_2k_results['acc_1']
        print(f"{'å‡†ç¡®ç‡â‰¤1':<15} {model_2k_results['acc_1']:<12.1f}% {model_5k_results['acc_1']:<12.1f}% {acc1_improvement:+.1f}%")
        
        acc2_improvement = model_5k_results['acc_2'] - model_2k_results['acc_2']
        print(f"{'å‡†ç¡®ç‡â‰¤2':<15} {model_2k_results['acc_2']:<12.1f}% {model_5k_results['acc_2']:<12.1f}% {acc2_improvement:+.1f}%")
        
        # è®­ç»ƒæ—¶é—´å¯¹æ¯”
        time_ratio = model_5k_results['training_time'] / model_2k_results['training_time']
        print(f"{'è®­ç»ƒæ—¶é—´':<15} {model_2k_results['training_time']:<12.1f}min {model_5k_results['training_time']:<12.1f}min {time_ratio:.1f}x")
        
        print("="*60)
        
        return results_5k

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹5Kæ¨¡å‹æ€§èƒ½æµ‹è¯•")
    
    # é…ç½®è·¯å¾„
    model_path = '/Users/huangxinyue/Multi model distillation/fast_models_5k/fast_best_model.pth'
    csv_file = '/Users/huangxinyue/Multi model distillation/train_10k_results/train_10k_fast_results.csv'
    database_path = '/Users/huangxinyue/Downloads/Influencer brand database'
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        logger.error(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = Model5KEvaluator(model_path, csv_file, database_path)
    
    # åŠ è½½æ¨¡å‹
    evaluator.load_model()
    
    # ä¸2Kæ¨¡å‹å¯¹æ¯”è¯„ä¼°
    results = evaluator.compare_with_2k_model()
    
    # ç”Ÿæˆè¯„ä¼°å›¾è¡¨
    evaluator.create_evaluation_plots(results)
    
    logger.info("âœ… 5Kæ¨¡å‹è¯„ä¼°å®Œæˆ")

if __name__ == "__main__":
    main()
