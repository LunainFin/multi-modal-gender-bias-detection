#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Google Colabç‰ˆæœ¬ - Instagramæ€§åˆ«å€¾å‘å¤šæ¨¡æ€æ¨¡å‹è®­ç»ƒ
ä¼˜åŒ–é…ç½®ï¼ŒGPUåŠ é€Ÿ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import json
import os
import numpy as np
from PIL import Image
import logging
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from transformers import AutoTokenizer, AutoModel
import timm
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
import sys
warnings.filterwarnings('ignore')

# è®¾ç½®ç¯å¢ƒå˜é‡é¿å…tokenizerè­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('colab_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Google Colab GPUæ£€æµ‹
def check_gpu():
    """æ£€æŸ¥GPUå¯ç”¨æ€§"""
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name()
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        logger.info(f"ğŸš€ GPUå¯ç”¨: {gpu_name}")
        logger.info(f"ğŸ’¾ æ˜¾å­˜: {gpu_memory:.1f} GB")
        return True
    else:
        logger.warning("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
        return False

class ColabInstagramDataset(Dataset):
    """Colabç‰ˆæœ¬çš„Instagramæ•°æ®é›†"""
    
    def __init__(self, csv_file, image_dir, tokenizer, max_length=128, image_size=224):
        # è¯»å–CSVï¼Œå¼ºåˆ¶post_idä¸ºå­—ç¬¦ä¸²
        self.df = pd.read_csv(csv_file, dtype={'post_id': str})
        self.image_dir = image_dir
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # å›¾åƒé¢„å¤„ç†
        from torchvision import transforms
        self.transform = transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        # è¿‡æ»¤æ‰æ— æ•ˆæ ·æœ¬
        self.valid_samples = []
        for idx, row in self.df.iterrows():
            post_id = str(row['post_id'])
            score = row['gender_bias_score']
            
            # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            img_path = os.path.join(image_dir, f"{post_id}.jpg")
            if os.path.exists(img_path) and not pd.isna(score):
                # æ·»åŠ é»˜è®¤captionï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                self.valid_samples.append({
                    'post_id': post_id,
                    'image_path': img_path,
                    'caption': f"Instagram post {post_id}",  # ç®€åŒ–caption
                    'score': float(score) / 10.0  # æ ‡å‡†åŒ–åˆ°0-1
                })
        
        logger.info(f"âœ… æœ‰æ•ˆæ ·æœ¬æ•°: {len(self.valid_samples)}")
    
    def __len__(self):
        return len(self.valid_samples)
    
    def __getitem__(self, idx):
        sample = self.valid_samples[idx]
        
        # åŠ è½½å›¾åƒ
        try:
            image = Image.open(sample['image_path']).convert('RGB')
            image = self.transform(image)
        except Exception as e:
            logger.warning(f"å›¾åƒåŠ è½½å¤±è´¥ {sample['post_id']}: {e}")
            # è¿”å›ç©ºç™½å›¾åƒ
            image = torch.zeros(3, 224, 224)
        
        # å¤„ç†æ–‡æœ¬
        caption = sample['caption']
        encoding = self.tokenizer(
            caption,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'image': image,
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'score': torch.tensor(sample['score'], dtype=torch.float32)
        }

class ColabGenderBiasModel(nn.Module):
    """Colabä¼˜åŒ–ç‰ˆå¤šæ¨¡æ€æ¨¡å‹"""
    
    def __init__(self, 
                 image_model='resnet18',
                 text_model='distilbert-base-uncased',
                 hidden_dim=256,
                 dropout_rate=0.3):
        super().__init__()
        
        # å›¾åƒç¼–ç å™¨ (é¢„è®­ç»ƒResNet18)
        self.image_encoder = timm.create_model(image_model, pretrained=True, num_classes=0)
        image_dim = self.image_encoder.num_features
        
        # æ–‡æœ¬ç¼–ç å™¨ (DistilBERT)
        self.text_encoder = AutoModel.from_pretrained(text_model)
        text_dim = self.text_encoder.config.hidden_size
        
        # ç‰¹å¾èåˆå±‚
        fusion_dim = image_dim + text_dim
        self.fusion = nn.Sequential(
            nn.Linear(fusion_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()  # è¾“å‡º0-1èŒƒå›´
        )
        
        # å†»ç»“æ–‡æœ¬ç¼–ç å™¨éƒ¨åˆ†å±‚ä»¥åŠ é€Ÿè®­ç»ƒ
        for param in self.text_encoder.embeddings.parameters():
            param.requires_grad = False
    
    def forward(self, images, input_ids, attention_mask):
        # å›¾åƒç‰¹å¾
        image_features = self.image_encoder(images)
        
        # æ–‡æœ¬ç‰¹å¾
        text_outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)
        text_features = text_outputs.last_hidden_state.mean(dim=1)  # å¹³å‡æ± åŒ–
        
        # ç‰¹å¾èåˆ
        combined = torch.cat([image_features, text_features], dim=1)
        fused = self.fusion(combined)
        
        # é¢„æµ‹åˆ†æ•°
        scores = self.classifier(fused).squeeze()
        
        return scores

class ColabTrainer:
    """Colabè®­ç»ƒå™¨"""
    
    def __init__(self, 
                 csv_file='./train_10k_fast_results.csv',
                 image_dir='./images',
                 batch_size=32,          # GPUå¯ä»¥ç”¨æ›´å¤§batch
                 learning_rate=1e-3,     # ç¨é«˜çš„å­¦ä¹ ç‡
                 num_epochs=10,          # å‡å°‘epochæ•°
                 test_size=0.2):
        
        self.csv_file = csv_file
        self.image_dir = image_dir
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.num_epochs = num_epochs
        self.test_size = test_size
        
        # è®¾å¤‡æ£€æµ‹
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
        
        # è®­ç»ƒå†å²
        self.history = {
            'epoch': [],
            'train_loss': [],
            'val_loss': [],
            'val_mae': [],
            'val_r2': []
        }
    
    def prepare_data(self):
        """å‡†å¤‡æ•°æ®"""
        logger.info("ğŸ“Š å‡†å¤‡æ•°æ®...")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = ColabInstagramDataset(
            csv_file=self.csv_file,
            image_dir=self.image_dir,
            tokenizer=self.tokenizer
        )
        
        # åˆ†å‰²æ•°æ®
        train_size = int((1 - self.test_size) * len(dataset))
        val_size = len(dataset) - train_size
        self.train_dataset, self.val_dataset = torch.utils.data.random_split(
            dataset, [train_size, val_size]
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(
            self.train_dataset, 
            batch_size=self.batch_size, 
            shuffle=True,
            num_workers=2,  # Colabé€šå¸¸æœ‰2ä¸ªCPUæ ¸å¿ƒ
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        self.val_loader = DataLoader(
            self.val_dataset, 
            batch_size=self.batch_size, 
            shuffle=False,
            num_workers=2,
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        logger.info(f"âœ… è®­ç»ƒé›†: {len(self.train_dataset)} æ ·æœ¬")
        logger.info(f"âœ… éªŒè¯é›†: {len(self.val_dataset)} æ ·æœ¬")
    
    def create_model(self):
        """åˆ›å»ºæ¨¡å‹"""
        logger.info("ğŸ¤– åˆ›å»ºæ¨¡å‹...")
        
        self.model = ColabGenderBiasModel().to(self.device)
        
        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        self.criterion = nn.MSELoss()
        self.optimizer = optim.AdamW(
            self.model.parameters(), 
            lr=self.learning_rate, 
            weight_decay=1e-4
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, 
            mode='min', 
            factor=0.5, 
            patience=2,  # æ›´æ¿€è¿›çš„å­¦ä¹ ç‡è¡°å‡
            min_lr=1e-6
        )
        
        # æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        logger.info(f"ğŸ“Š æ€»å‚æ•°é‡: {total_params:,}")
        logger.info(f"ğŸ“Š å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    def train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        num_batches = len(self.train_loader)
        
        for batch_idx, batch in enumerate(tqdm(self.train_loader, desc='Training')):
            try:
                # æ•°æ®ç§»è‡³GPU
                images = batch['image'].to(self.device)
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                targets = batch['score'].to(self.device)
                
                # å‰å‘ä¼ æ’­
                self.optimizer.zero_grad()
                outputs = self.model(images, input_ids, attention_mask)
                
                # ç¡®ä¿ç»´åº¦åŒ¹é…
                if outputs.dim() == 0:
                    outputs = outputs.unsqueeze(0)
                if targets.dim() == 0:
                    targets = targets.unsqueeze(0)
                
                loss = self.criterion(outputs, targets)
                
                # åå‘ä¼ æ’­
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()
                
                total_loss += loss.item()
                
            except Exception as e:
                logger.warning(f"è®­ç»ƒæ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
                continue
        
        return total_loss / num_batches if num_batches > 0 else 0
    
    def validate(self):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc='Validation'):
                try:
                    images = batch['image'].to(self.device)
                    input_ids = batch['input_ids'].to(self.device)
                    attention_mask = batch['attention_mask'].to(self.device)
                    targets = batch['score'].to(self.device)
                    
                    outputs = self.model(images, input_ids, attention_mask)
                    
                    # å¤„ç†ç»´åº¦
                    if outputs.dim() == 0:
                        outputs = outputs.unsqueeze(0)
                    if targets.dim() == 0:
                        targets = targets.unsqueeze(0)
                    
                    loss = self.criterion(outputs, targets)
                    total_loss += loss.item()
                    
                    # è½¬æ¢å›åŸå§‹åˆ†æ•°èŒƒå›´[0,10]
                    predictions = outputs.cpu().numpy() * 10.0
                    targets_np = targets.cpu().numpy() * 10.0
                    
                    all_predictions.extend(predictions)
                    all_targets.extend(targets_np)
                    
                except Exception as e:
                    logger.warning(f"éªŒè¯æ‰¹æ¬¡å¤±è´¥: {e}")
                    continue
        
        avg_loss = total_loss / len(self.val_loader) if len(self.val_loader) > 0 else 0
        
        if all_predictions and all_targets:
            mae = mean_absolute_error(all_targets, all_predictions)
            r2 = r2_score(all_targets, all_predictions)
        else:
            mae = 0
            r2 = 0
        
        return avg_loss, mae, r2, all_predictions, all_targets
    
    def save_model(self, epoch, is_best=False):
        """ä¿å­˜æ¨¡å‹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'history': self.history
        }
        
        # ä¿å­˜æœ€æ–°æ¨¡å‹
        torch.save(checkpoint, 'latest_model.pth')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            torch.save(checkpoint, 'best_model.pth')
            logger.info("ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹")
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹Colabè®­ç»ƒ...")
        
        # å‡†å¤‡æ•°æ®
        self.prepare_data()
        
        # åˆ›å»ºæ¨¡å‹
        self.create_model()
        
        best_val_loss = float('inf')
        patience_counter = 0
        patience = 3  # æ›´æ¿€è¿›çš„æ—©åœ
        
        for epoch in range(1, self.num_epochs + 1):
            logger.info(f"\nğŸ”„ Epoch {epoch}/{self.num_epochs}")
            
            # è®­ç»ƒ
            train_loss = self.train_epoch()
            
            # éªŒè¯
            val_loss, val_mae, val_r2, predictions, targets = self.validate()
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(val_loss)
            
            # è®°å½•å†å²
            self.history['epoch'].append(epoch)
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            self.history['val_mae'].append(val_mae)
            self.history['val_r2'].append(val_r2)
            
            # æ£€æŸ¥æœ€ä½³æ¨¡å‹
            is_best = val_loss < best_val_loss
            if is_best:
                best_val_loss = val_loss
                patience_counter = 0
            else:
                patience_counter += 1
            
            # ä¿å­˜æ¨¡å‹
            self.save_model(epoch, is_best)
            
            # è¾“å‡ºç»“æœ
            logger.info(f"ğŸ“Š Train Loss: {train_loss:.4f}")
            logger.info(f"ğŸ“Š Val Loss: {val_loss:.4f}, MAE: {val_mae:.4f}, RÂ²: {val_r2:.4f}")
            logger.info(f"âš™ï¸  å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.2e}")
            
            # æ—©åœ
            if patience_counter >= patience:
                logger.info(f"â¹ï¸  éªŒè¯æŸå¤±åœ¨{patience}ä¸ªepochå†…æ²¡æœ‰æ”¹å–„ï¼Œæå‰åœæ­¢")
                break
        
        logger.info("ğŸ‰ Colabè®­ç»ƒå®Œæˆï¼")
        logger.info(f"ğŸ† æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves()
    
    def plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        
        # Lossæ›²çº¿
        axes[0,0].plot(self.history['epoch'], self.history['train_loss'], 'b-', label='Train Loss')
        axes[0,0].plot(self.history['epoch'], self.history['val_loss'], 'r-', label='Val Loss')
        axes[0,0].set_title('Lossæ›²çº¿')
        axes[0,0].set_xlabel('Epoch')
        axes[0,0].set_ylabel('Loss')
        axes[0,0].legend()
        axes[0,0].grid(True)
        
        # MAEæ›²çº¿
        axes[0,1].plot(self.history['epoch'], self.history['val_mae'], 'g-', label='Val MAE')
        axes[0,1].set_title('å¹³å‡ç»å¯¹è¯¯å·®')
        axes[0,1].set_xlabel('Epoch')
        axes[0,1].set_ylabel('MAE')
        axes[0,1].legend()
        axes[0,1].grid(True)
        
        # RÂ²æ›²çº¿
        axes[1,0].plot(self.history['epoch'], self.history['val_r2'], 'm-', label='Val RÂ²')
        axes[1,0].set_title('å†³å®šç³»æ•°')
        axes[1,0].set_xlabel('Epoch')
        axes[1,0].set_ylabel('RÂ²')
        axes[1,0].legend()
        axes[1,0].grid(True)
        
        # éšè—æœ€åä¸€ä¸ªå­å›¾
        axes[1,1].axis('off')
        
        plt.tight_layout()
        plt.savefig('colab_training_curves.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        logger.info("ğŸ“ˆ è®­ç»ƒæ›²çº¿å·²ä¿å­˜ä¸º colab_training_curves.png")

def main():
    """ä¸»å‡½æ•°"""
    
    # GPUæ£€æµ‹
    check_gpu()
    
    # é…ç½®Colabä¼˜åŒ–è®­ç»ƒ
    trainer = ColabTrainer(
        csv_file='./train_10k_fast_results.csv',
        image_dir='./images',
        batch_size=64 if torch.cuda.is_available() else 16,  # GPUç”¨å¤§batch
        learning_rate=1e-3,      # æé«˜å­¦ä¹ ç‡
        num_epochs=10,           # å‡å°‘è½®æ•°
        test_size=0.2
    )
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()

if __name__ == "__main__":
    main()






